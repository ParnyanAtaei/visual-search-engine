{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5186786,"sourceType":"datasetVersion","datasetId":3015609}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install requirements","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T15:09:17.620983Z","iopub.execute_input":"2025-07-14T15:09:17.622244Z","iopub.status.idle":"2025-07-14T15:09:23.801101Z","shell.execute_reply.started":"2025-07-14T15:09:17.622218Z","shell.execute_reply":"2025-07-14T15:09:23.800221Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.11.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import library","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nimport faiss\nimport gradio as gr\nimport pickle\nfrom transformers import ViTFeatureExtractor, ViTModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T15:09:38.366365Z","iopub.execute_input":"2025-07-14T15:09:38.367075Z","iopub.status.idle":"2025-07-14T15:10:07.271740Z","shell.execute_reply.started":"2025-07-14T15:09:38.367009Z","shell.execute_reply":"2025-07-14T15:10:07.271034Z"}},"outputs":[{"name":"stderr","text":"2025-07-14 15:09:53.031391: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752505793.226193      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752505793.281006      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Settings\nIMAGE_FOLDER = \"/kaggle/input/2017-2017/train2017/train2017\"\nFEATURES_PATH = \"vit_image_features.npy\"\nPATHS_PATH = \"vit_image_paths.pkl\"\nBATCH_SIZE = 16\nTOP_K = 5\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T15:10:19.373629Z","iopub.execute_input":"2025-07-14T15:10:19.374741Z","iopub.status.idle":"2025-07-14T15:10:19.378606Z","shell.execute_reply.started":"2025-07-14T15:10:19.374715Z","shell.execute_reply":"2025-07-14T15:10:19.377751Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"The optimal batch size can differ between CNN-based and ViT-based models due to differences in their architecture, memory usage, and computational requirements.\nCNNs (e.g., ResNet): Typically have fewer parameters and require less memory per image. They can often handle larger batch sizes on the same hardware.\nViTs (Vision Transformers): Use self-attention, which has memory and compute requirements that scale quadratically with image size and linearly with batch size. Each image is split into many patches, and attention is computed between all pairs of patches, consuming more memory.\nAs a result, ViTs often require smaller batch sizes to avoid running out of GPU/CPU memory.\nMoreover, ViTs often use larger input sizes (e.g., 224x224 or 384x384) and more complex preprocessing, which can further increase memory usage.","metadata":{}},{"cell_type":"markdown","source":"# Load ViT model and feature extractor","metadata":{}},{"cell_type":"code","source":"# Load ViT model and feature extractor\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\nvit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k').to(device)\nvit_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T15:10:21.505620Z","iopub.execute_input":"2025-07-14T15:10:21.506138Z","iopub.status.idle":"2025-07-14T15:10:24.640529Z","shell.execute_reply.started":"2025-07-14T15:10:21.506114Z","shell.execute_reply":"2025-07-14T15:10:24.639911Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87459a5f025145a8a5aceadc1104935b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"291cf7462511433bb7c31fc3d5dd81ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a509ea078fa241a0bf99ac30720e354a"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"ViTModel(\n  (embeddings): ViTEmbeddings(\n    (patch_embeddings): ViTPatchEmbeddings(\n      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (dropout): Dropout(p=0.0, inplace=False)\n  )\n  (encoder): ViTEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x ViTLayer(\n        (attention): ViTAttention(\n          (attention): ViTSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output): ViTSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (intermediate): ViTIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ViTOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (pooler): ViTPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Feature extraction function","metadata":{}},{"cell_type":"code","source":"\nimage_paths = [os.path.join(IMAGE_FOLDER, fname) for fname in os.listdir(IMAGE_FOLDER)\n               if fname.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\ndef extract_vit_features(image_paths, batch_size=BATCH_SIZE):\n    features = []\n    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Extracting ViT features\"):\n        batch_paths = image_paths[i:i+batch_size]\n        images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n        inputs = feature_extractor(images=images, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = vit_model(**inputs)\n            batch_features = outputs.last_hidden_state[:, 0, :]  # CLS token\n            batch_features = batch_features / batch_features.norm(dim=-1, keepdim=True)\n        features.append(batch_features.cpu().numpy())\n    return np.concatenate(features, axis=0).astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T15:10:32.786912Z","iopub.execute_input":"2025-07-14T15:10:32.787200Z","iopub.status.idle":"2025-07-14T15:10:33.899071Z","shell.execute_reply.started":"2025-07-14T15:10:32.787177Z","shell.execute_reply":"2025-07-14T15:10:33.898507Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"This function iterates through your collection of images in batches, preprocesses them using feature_extractor, feeds them through the vit_model to get their feature vectors, and then normalizes these features.","metadata":{}},{"cell_type":"code","source":"# Load or compute features\nif os.path.exists(FEATURES_PATH) and os.path.exists(PATHS_PATH):\n    image_features = np.load(FEATURES_PATH)\n    with open(PATHS_PATH, \"rb\") as f:\n        saved_paths = pickle.load(f)\n    if set(saved_paths) != set(image_paths):\n        print(\"Image set changed, re-extracting features...\")\n        image_features = extract_vit_features(image_paths)\n        np.save(FEATURES_PATH, image_features)\n        with open(PATHS_PATH, \"wb\") as f:\n            pickle.dump(image_paths, f)\nelse:\n    image_features = extract_vit_features(image_paths)\n    np.save(FEATURES_PATH, image_features)\n    with open(PATHS_PATH, \"wb\") as f:\n        pickle.dump(image_paths, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T15:10:37.187750Z","iopub.execute_input":"2025-07-14T15:10:37.188020Z","iopub.status.idle":"2025-07-14T16:02:04.988557Z","shell.execute_reply.started":"2025-07-14T15:10:37.187998Z","shell.execute_reply":"2025-07-14T16:02:04.987923Z"}},"outputs":[{"name":"stderr","text":"Extracting ViT features: 100%|██████████| 7393/7393 [51:26<00:00,  2.39it/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# FAISS index\nInitializes a FAISS index (IndexFlatIP) designed for cosine similarity search (because your features are L2-normalized) and then populates it with all the extracted features from image collection. ","metadata":{}},{"cell_type":"code","source":"# Build FAISS index\nindex = faiss.IndexFlatIP(image_features.shape[1])\nindex.add(image_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T16:02:13.268239Z","iopub.execute_input":"2025-07-14T16:02:13.268496Z","iopub.status.idle":"2025-07-14T16:02:13.607358Z","shell.execute_reply.started":"2025-07-14T16:02:13.268476Z","shell.execute_reply":"2025-07-14T16:02:13.606753Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Search functions and Gradio-based demo","metadata":{}},{"cell_type":"code","source":"def search_by_image_vit(query_image, top_k=TOP_K):\n    image = query_image.convert(\"RGB\")\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = vit_model(**inputs)\n        image_features_query = outputs.last_hidden_state[:, 0, :]\n        image_features_query = image_features_query / image_features_query.norm(dim=-1, keepdim=True)\n    image_features_query = image_features_query.cpu().numpy().astype(\"float32\")\n    D, I = index.search(image_features_query, top_k)\n    return [image_paths[i] for i in I[0]]\n\ndef visual_search_vit(image_query):\n    if image_query is not None:\n        results = search_by_image_vit(image_query)\n        return [Image.open(p) for p in results]\n    else:\n        return []\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# ViT Visual Search Engine (Image-to-Image)\")\n    image_input = gr.Image(type=\"pil\", label=\"Upload an image to search\")\n    output_gallery = gr.Gallery(label=\"Top Results\", columns=5, height=\"auto\")\n    search_btn = gr.Button(\"Search\")\n    search_btn.click(\n        fn=visual_search_vit,\n        inputs=image_input,\n        outputs=output_gallery\n    )\n\ndemo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T16:02:17.101200Z","iopub.execute_input":"2025-07-14T16:02:17.101487Z","iopub.status.idle":"2025-07-14T16:02:18.538602Z","shell.execute_reply.started":"2025-07-14T16:02:17.101464Z","shell.execute_reply":"2025-07-14T16:02:18.538052Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://5e58bc1aae84fc220f.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://5e58bc1aae84fc220f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}