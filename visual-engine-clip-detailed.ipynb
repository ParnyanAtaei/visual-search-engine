{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4643634,"sourceType":"datasetVersion","datasetId":2699575},{"sourceId":5186786,"sourceType":"datasetVersion","datasetId":3015609}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install requirements\n\n### Faiss & CLIP:\n\n**Faiss:** Based on [Faiss paper](https://arxiv.org/abs/2401.08281), Faiss, which stands for \"Facebook AI Similarity Search,\" is a powerful and efficient library for similarity search and similarity indexing. Optimized for search through millions or billions of heigh-dimenstion vectors quickly. Key features and characteristics of Faiss include:\n\n1. Efficient Vector Search: Faiss is optimized for fast similarity search in large datasets of high-dimensional vectors. It provides both exact and approximate search algorithms, making it suitable for a wide range of use cases.\n2. GPU Support: Faiss includes GPU support, allowing users to take advantage of the computational power of modern graphics processing units to accelerate similarity search operations.\n3. Diverse Indexing Structures: Faiss provides a variety of indexing structures, including flat indexes, IVF (Inverted File) indexes, HNSW (Hierarchical Navigable Small World) indexes, and more, each tailored to specific data and performance requirements. \n4. Integration with Deep Learning: Faiss is often used in conjunction with deep learning models, making it a valuable tool for encoding text, images, and other data into high-dimensional vectors. These vectors can then be efficiently searched and indexed.\n5. Wide Range of Applications: Faiss is used in various applications, including content-based recommendation systems, similarity-based search engines, image retrieval, and document clustering, to name a few.\n6. Scalability: Faiss is designed to handle large datasets, making it suitable for both small-scale projects and large-scale production systems.\n\n\n**CLIP:** The CLIP (Contrastive Language-Image Pre-training) model, developed by OpenAI, is a multi-modal vision and language model that maps images and text to the same latent space. Since we will use both image and text queries to search for images, we will use the CLIP model to embed our data. \n","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu  git+https://github.com/openai/CLIP.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T16:28:37.775340Z","iopub.execute_input":"2025-07-16T16:28:37.775525Z","iopub.status.idle":"2025-07-16T16:29:58.607480Z","shell.execute_reply.started":"2025-07-16T16:28:37.775508Z","shell.execute_reply":"2025-07-16T16:29:58.606539Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-75ffq4v6\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-75ffq4v6\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=06b6a3658730aa7a9d7766ccc198510b0431bd38cf3b5fb8c85ea4bff0d6a415\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ruay13iw/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, faiss-cpu, clip\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed clip-1.0 faiss-cpu-1.11.0.post1 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import library","metadata":{}},{"cell_type":"code","source":"import os\nimport clip\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport faiss\nfrom tqdm import tqdm\nimport gradio as gr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T16:29:58.611251Z","iopub.execute_input":"2025-07-16T16:29:58.611454Z","iopub.status.idle":"2025-07-16T16:30:10.459456Z","shell.execute_reply.started":"2025-07-16T16:29:58.611430Z","shell.execute_reply":"2025-07-16T16:30:10.458895Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Load CLIP model\n\nThe model uses a **ViT-B/32** Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T16:30:10.460850Z","iopub.execute_input":"2025-07-16T16:30:10.461278Z","iopub.status.idle":"2025-07-16T16:30:19.447513Z","shell.execute_reply.started":"2025-07-16T16:30:10.461259Z","shell.execute_reply":"2025-07-16T16:30:19.446933Z"}},"outputs":[{"name":"stderr","text":"100%|███████████████████████████████████████| 338M/338M [00:04<00:00, 88.1MiB/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Prepare image dataset\nImage collection is train folder of COCO 2017 dataset","metadata":{}},{"cell_type":"code","source":"image_folder = \"/kaggle/input/coco25k/images\"\nimage_paths = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder) if fname.lower().endswith(('.png', '.jpg', '.jpeg'))]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T16:30:19.448265Z","iopub.execute_input":"2025-07-16T16:30:19.448513Z","iopub.status.idle":"2025-07-16T16:30:19.691096Z","shell.execute_reply.started":"2025-07-16T16:30:19.448487Z","shell.execute_reply":"2025-07-16T16:30:19.690546Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Extract features \nThis block extracts high-dimensional numerical \"feature vectors\" (embeddings) for every image in your collection using the CLIP model.\n\nFirst preprocess the image using the preprocess function we got earlier. This performs a few things to ensure the input to the CLIP model is of the right format and dimensionality including resizing, normalization, colour channel adjustment etc.","metadata":{}},{"cell_type":"code","source":"#Extract features \nfeatures_path = \"image_features.npy\"\nif os.path.exists(features_path):\n    image_features = np.load(features_path)\nelse:\n    image_features = []\n    for path in tqdm(image_paths, desc=\"Extracting image features\"):\n        image = preprocess(Image.open(path)).unsqueeze(0).to(device)\n        with torch.no_grad():\n            feature = model.encode_image(image)\n            feature /= feature.norm(dim=-1, keepdim=True)\n            image_features.append(feature.cpu().numpy())\n    image_features = np.concatenate(image_features, axis=0).astype(\"float32\")\n    np.save(features_path, image_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T16:30:22.692485Z","iopub.execute_input":"2025-07-16T16:30:22.692774Z","iopub.status.idle":"2025-07-16T16:39:06.116835Z","shell.execute_reply.started":"2025-07-16T16:30:22.692754Z","shell.execute_reply":"2025-07-16T16:39:06.116202Z"}},"outputs":[{"name":"stderr","text":"Extracting image features: 100%|██████████| 25000/25000 [08:43<00:00, 47.77it/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# FAISS index\nInitializes a FAISS index (IndexFlatIP) designed for cosine similarity search (because your features are L2-normalized) and then populates it with all the extracted CLIP features from your image collection. \n\n### IndexFlatIP:\n\nA flat index takes your high-dimensional feature vectors and stores them exactly as they are. It is one of the simplest index structure where all data points are stored without any transformation (compression). This type of index doesn’t compress or cluster your vectors. Flat indexes are ‘flat’ because they do not modify the vectors that we feed into them.\n\nBecause there is no approximation or clustering of vectors — these indexes produce the most accurate results. We have perfect search quality, but this comes at the cost of significant search times.\n\nWith flat indexes, we introduce our query vector xq and compare it against every other full-size vector in our index — calculating the distance/inner-product to each. This is an EXHAUSTIVE SERACH.\n\nAfter calculating all of these distances, we will return the nearest k of those as our nearest matches. A k-nearest neighbors (kNN) search.\n\nAnd for flat indexes, that is all we need to do — there is no training (as we have no parameters to optimize when storing vectors without transformations or clustering).\n\nWhen To Use:\n\n* Search quality is a very high priority.\n\n* Search time does not matter OR when using a small index (<10K)","metadata":{}},{"cell_type":"code","source":"#FAISS index\nindex = faiss.IndexFlatIP(image_features.shape[1])\nindex.add(image_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T16:39:12.422166Z","iopub.execute_input":"2025-07-16T16:39:12.422759Z","iopub.status.idle":"2025-07-16T16:39:12.474323Z","shell.execute_reply.started":"2025-07-16T16:39:12.422729Z","shell.execute_reply":"2025-07-16T16:39:12.473597Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Search functions\nAfter a query image or text is encoded by the model's encoder, the resulting embedding must be normalized for inner product search through other image embeddings. \n\n\n### Normalization\n\nVector normalization ensures all vectors have a magnitude of 1, which simplifies the computation of similarity metrics.\n\n* **Cosine similarity** inherently measures the angle between vectors, ignoring their magnitudes. Normalization simplifies the calculation of Cosine similarity to a dot product.\n\n* By normalization, both images are compared purely by direction (cosine similarity), prioritizing semantic relevance over pixel intensity. \n\n* Without normalization, a high-resolution image (large vector magnitude) might appear “closer” to a query vector than a semantically similar low-resolution image due to Euclidean distance favoring magnitude.\n\n**Semantic Meaning:** In the context of embeddings (like those from CLIP or ViT), the direction of the vector in the high-dimensional space often represents its semantic meaning or content. For example, all embeddings for \"cats\" might point generally in one direction, while all embeddings for \"dogs\" point in another. The length of the vector might represent other less important factors (like how common the object is, or the \"intensity\" of the image). By ignoring magnitude, cosine similarity ensures that two items are compared purely on their conceptual or visual similarity, not on incidental differences in their numerical \"strength\" or \"loudness.\" This makes the search results more relevant to what the user actually means.\n\n","metadata":{}},{"cell_type":"code","source":"#Search functions\ndef search_by_text(query, top_k=5):\n    text = clip.tokenize([query]).to(device)\n    with torch.no_grad():\n        text_features = model.encode_text(text)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n    text_features = text_features.cpu().numpy().astype(\"float32\")\n    D, I = index.search(text_features, top_k)\n    return [image_paths[i] for i in I[0]]\n\ndef search_by_image(query_image, top_k=5):\n    image = preprocess(query_image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        image_features_query = model.encode_image(image)\n        image_features_query /= image_features_query.norm(dim=-1, keepdim=True)\n    image_features_query = image_features_query.cpu().numpy().astype(\"float32\")\n    D, I = index.search(image_features_query, top_k)\n    return [image_paths[i] for i in I[0]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T16:39:23.547967Z","iopub.execute_input":"2025-07-16T16:39:23.548659Z","iopub.status.idle":"2025-07-16T16:39:23.554389Z","shell.execute_reply.started":"2025-07-16T16:39:23.548633Z","shell.execute_reply":"2025-07-16T16:39:23.553591Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"The Retrieval is happening on the index.search method. It implements a k-Nearest Neighbors (kNN) search to find the k most similar vectors to the query vector. We can adjust the value of k by changing the top_k parameter. The distance metric used in the kNN search in our implementation is the cosine similarity. The function returns a list of retrieve images paths.","metadata":{}},{"cell_type":"markdown","source":"# Gradio\nBuilt a Gradio-based visual search demo to display the top 5 images similar to a given text or image query.","metadata":{}},{"cell_type":"code","source":"#Gradio \ndef visual_search(text_query, image_query):\n    if text_query:\n        results = search_by_text(text_query)\n    elif image_query is not None:\n        results = search_by_image(image_query)\n    else:\n        return []\n    return [Image.open(p) for p in results]\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# CLIP Visual Search Engine\")\n    with gr.Row():\n        text_input = gr.Textbox(label=\"Text Query\", placeholder=\"Describe the image you want to find...\")\n        image_input = gr.Image(type=\"pil\", label=\"Or upload an image\")\n    output_gallery = gr.Gallery(label=\"Top Results\", columns=5, height=\"auto\")\n    search_btn = gr.Button(\"Search\")\n    search_btn.click(\n        fn=visual_search,\n        inputs=[text_input, image_input],\n        outputs=output_gallery\n    )\n\ndemo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T16:39:28.332651Z","iopub.execute_input":"2025-07-16T16:39:28.333236Z","iopub.status.idle":"2025-07-16T16:39:29.556897Z","shell.execute_reply.started":"2025-07-16T16:39:28.333211Z","shell.execute_reply":"2025-07-16T16:39:29.556303Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://6c08a9446c6f126edb.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://6c08a9446c6f126edb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## How can we make our search faster?\n\nThere are two primary approaches:\n\n**Reduce vector size** — through dimensionality reduction or reducing the number of bits representing our vectors values.\n\n**Reduce search scope** — we can do this by clustering or organizing vectors into tree structures based on certain attributes, similarity, or distance — and restricting our search to closest clusters or filter through most similar branches.\n\nUsing either of these approaches means that we are no longer performing an exhaustive nearest-neighbors search but an approximate nearest-neighbors (ANN) search — as we no longer search the entire, full-resolution dataset. So, what we produce is a more balanced mix that prioritizes both search-speed and search-time.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}